{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414eaa16-2b95-459f-9221-f50f7749f35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Installing collected packages: safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.12.0 huggingface-hub-0.27.0 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Installing collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1 triton-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.19,>=2.18 (from tf-keras)\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading wrapt-1.17.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading grpcio-1.68.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading optree-0.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.68.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "Downloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "Downloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.17.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow, tf-keras\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.1\n",
      "    Uninstalling numpy-2.2.1:\n",
      "      Successfully uninstalled numpy-2.2.1\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.12.23 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.1 h5py-3.12.1 keras-3.7.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.13.1 protobuf-5.29.2 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0 tf-keras-2.18.0 werkzeug-3.1.3 wrapt-1.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.12/site-packages (from scipy) (2.0.2)\n",
      "Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (5.29.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sentence-transformers\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.0 sentence-transformers-3.3.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m689.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# libraries🔥\n",
    "\n",
    "# install🔥\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install tf-keras # install this because keras needs to work on a previous version of python\n",
    "%pip install scipy\n",
    "%pip install pandas\n",
    "%pip install tensorflow\n",
    "%pip install -U sentence-transformers\n",
    "%pip install numpy\n",
    "%pip install sentencepiece\n",
    "# %pip install transformers[sentencepiece] # didn't help, don't know if helpful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4caa56bf-804c-4ab6-be74-2a97928362ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import🔥\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertModel, TFBertModel # for BERT\n",
    "from transformers import AutoTokenizer, AutoModel # for DeBERTa\n",
    "from transformers import DebertaV2Tokenizer, AutoModel # does something but need sentencepiece\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e097c45-e060-4113-b514-3e2957166290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>words</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "      <th>leaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91719925</td>\n",
       "      <td>723.0</td>\n",
       "      <td>23-11-02</td>\n",
       "      <td>US</td>\n",
       "      <td>The Boston Globe</td>\n",
       "      <td>https://www.bostonglobe.com/2023/11/02/metro/d...</td>\n",
       "      <td>this band plans to drown out the men's march a...</td>\n",
       "      <td>kirk israel, an activist musician and member o...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91721731</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>23-11-03</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>https://www.foxnews.com/politics/virginias-ele...</td>\n",
       "      <td>virginia's elections a key 2024 barometer and ...</td>\n",
       "      <td>youngkin wore a similar red vest two years ago...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103177268</td>\n",
       "      <td>1947.0</td>\n",
       "      <td>23-11-03</td>\n",
       "      <td>US</td>\n",
       "      <td>inquisitr.com</td>\n",
       "      <td>https://www.theatlantic.com/ideas/archive/2023...</td>\n",
       "      <td>here's what biden can do to change his grim po...</td>\n",
       "      <td>whatever your theory, it should take into acco...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103177271</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>23-11-03</td>\n",
       "      <td>US</td>\n",
       "      <td>inquisitr.com</td>\n",
       "      <td>https://www.theatlantic.com/ideas/archive/2023...</td>\n",
       "      <td>don't equate anti-zionism with anti-semitism</td>\n",
       "      <td>on october 7, the islamist militant group hama...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103177279</td>\n",
       "      <td>2607.0</td>\n",
       "      <td>23-11-03</td>\n",
       "      <td>US</td>\n",
       "      <td>inquisitr.com</td>\n",
       "      <td>https://www.theatlantic.com/books/archive/2023...</td>\n",
       "      <td>do you have free will?</td>\n",
       "      <td>writing a review is an exercise in free will. ...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090</th>\n",
       "      <td>97824446</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>23-03-28</td>\n",
       "      <td>US</td>\n",
       "      <td>bostonglobe.com</td>\n",
       "      <td>https://bostonglobe.com/2023/03/26/metro/rev-e...</td>\n",
       "      <td>rev. elinor lockwood yeo, reproductive rights ...</td>\n",
       "      <td>rev. elinor lockwood yeo of newton, a reproduc...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8091</th>\n",
       "      <td>97847400</td>\n",
       "      <td>648.0</td>\n",
       "      <td>23-03-29</td>\n",
       "      <td>US</td>\n",
       "      <td>sfchronicle.com</td>\n",
       "      <td>https://sfchronicle.com/opinion/letterstotheed...</td>\n",
       "      <td>letters: how the country can respond to nashvi...</td>\n",
       "      <td>enough with the \" thoughts and prayers. \" thre...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8092</th>\n",
       "      <td>97848056</td>\n",
       "      <td>918.0</td>\n",
       "      <td>23-03-29</td>\n",
       "      <td>US</td>\n",
       "      <td>bostonglobe.com</td>\n",
       "      <td>https://bostonglobe.com/2023/03/28/opinion/abo...</td>\n",
       "      <td>the latest antiabortion tactic: asserting the ...</td>\n",
       "      <td>antiabortion activist trooper elwonger, 25, fe...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8093</th>\n",
       "      <td>90760423</td>\n",
       "      <td>1224.0</td>\n",
       "      <td>23-03-31</td>\n",
       "      <td>US</td>\n",
       "      <td>The Daily Beast</td>\n",
       "      <td>https://www.thedailybeast.com/satan-wants-you-...</td>\n",
       "      <td>inside the horrific (contested) abuse story th...</td>\n",
       "      <td>how did one discredited biography ignite one o...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8094</th>\n",
       "      <td>97893538</td>\n",
       "      <td>2247.0</td>\n",
       "      <td>23-03-31</td>\n",
       "      <td>US</td>\n",
       "      <td>theatlantic.com</td>\n",
       "      <td>https://www.theatlantic.com/politics/archive/2...</td>\n",
       "      <td>the first electoral test of trump's indictment</td>\n",
       "      <td>the most important election of 2023 may also o...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8095 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID   words      date country            source  \\\n",
       "0      91719925   723.0  23-11-02      US  The Boston Globe   \n",
       "1      91721731  1248.0  23-11-03      US          Fox News   \n",
       "2     103177268  1947.0  23-11-03      US     inquisitr.com   \n",
       "3     103177271  1513.0  23-11-03      US     inquisitr.com   \n",
       "4     103177279  2607.0  23-11-03      US     inquisitr.com   \n",
       "...         ...     ...       ...     ...               ...   \n",
       "8090   97824446  1244.0  23-03-28      US   bostonglobe.com   \n",
       "8091   97847400   648.0  23-03-29      US   sfchronicle.com   \n",
       "8092   97848056   918.0  23-03-29      US   bostonglobe.com   \n",
       "8093   90760423  1224.0  23-03-31      US   The Daily Beast   \n",
       "8094   97893538  2247.0  23-03-31      US   theatlantic.com   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://www.bostonglobe.com/2023/11/02/metro/d...   \n",
       "1     https://www.foxnews.com/politics/virginias-ele...   \n",
       "2     https://www.theatlantic.com/ideas/archive/2023...   \n",
       "3     https://www.theatlantic.com/ideas/archive/2023...   \n",
       "4     https://www.theatlantic.com/books/archive/2023...   \n",
       "...                                                 ...   \n",
       "8090  https://bostonglobe.com/2023/03/26/metro/rev-e...   \n",
       "8091  https://sfchronicle.com/opinion/letterstotheed...   \n",
       "8092  https://bostonglobe.com/2023/03/28/opinion/abo...   \n",
       "8093  https://www.thedailybeast.com/satan-wants-you-...   \n",
       "8094  https://www.theatlantic.com/politics/archive/2...   \n",
       "\n",
       "                                               headline  \\\n",
       "0     this band plans to drown out the men's march a...   \n",
       "1     virginia's elections a key 2024 barometer and ...   \n",
       "2     here's what biden can do to change his grim po...   \n",
       "3          don't equate anti-zionism with anti-semitism   \n",
       "4                                do you have free will?   \n",
       "...                                                 ...   \n",
       "8090  rev. elinor lockwood yeo, reproductive rights ...   \n",
       "8091  letters: how the country can respond to nashvi...   \n",
       "8092  the latest antiabortion tactic: asserting the ...   \n",
       "8093  inside the horrific (contested) abuse story th...   \n",
       "8094     the first electoral test of trump's indictment   \n",
       "\n",
       "                                                   body leaning  \n",
       "0     kirk israel, an activist musician and member o...    Left  \n",
       "1     youngkin wore a similar red vest two years ago...   Right  \n",
       "2     whatever your theory, it should take into acco...    Left  \n",
       "3     on october 7, the islamist militant group hama...    Left  \n",
       "4     writing a review is an exercise in free will. ...    Left  \n",
       "...                                                 ...     ...  \n",
       "8090  rev. elinor lockwood yeo of newton, a reproduc...    Left  \n",
       "8091  enough with the \" thoughts and prayers. \" thre...    Left  \n",
       "8092  antiabortion activist trooper elwonger, 25, fe...    Left  \n",
       "8093  how did one discredited biography ignite one o...    Left  \n",
       "8094  the most important election of 2023 may also o...    Left  \n",
       "\n",
       "[8095 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv\n",
    "file_path = \"/work/Bachelor/all_articles_allsides_abortion.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e85b93-f85d-48df-a04d-0a7ce9b4617f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3c9a193aa1489a9e65f881de9e9d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e28170fbb747eb8199fd29b775597c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1318a257d6b4779a483313e4011cd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d858f646134d9c961758982d6f4c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_496/125185392.py:105: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['year'] = pd.to_datetime(df['date']).dt.year  # Extract the year from the 'date' column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year  semantic_polarity\n",
      "0   2001           0.146246\n",
      "1   2002           0.163964\n",
      "2   2003           0.144314\n",
      "3   2004           0.146139\n",
      "4   2005           0.162829\n",
      "5   2006           0.179563\n",
      "6   2007           0.162130\n",
      "7   2008           0.169943\n",
      "8   2009           0.162576\n",
      "9   2010           0.166860\n",
      "10  2011           0.159802\n",
      "11  2012           0.173202\n",
      "12  2013           0.157998\n",
      "13  2014           0.152847\n",
      "14  2015           0.161099\n",
      "15  2016           0.155210\n",
      "16  2017           0.166148\n",
      "17  2018           0.152493\n",
      "18  2019           0.179476\n",
      "19  2020           0.173120\n",
      "20  2021           0.162476\n",
      "21  2022           0.148312\n",
      "22  2023           0.148737\n",
      "23  2024           0.145665\n",
      "24  2025           0.160252\n",
      "25  2026           0.174160\n",
      "26  2027           0.163742\n",
      "27  2028           0.167304\n",
      "28  2029           0.169089\n",
      "29  2030           0.163533\n",
      "30  2031           0.179741\n"
     ]
    }
   ],
   "source": [
    "# not fire\n",
    "\n",
    "# the one🔥 (years)\n",
    "\n",
    "# Load tokenizer and model\n",
    "# DeBERTa-v3-large is used for its advanced contextual understanding\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "model = AutoModel.from_pretrained('microsoft/deberta-v3-large')\n",
    "\n",
    "# Define the case-insensitive tokenizer function\n",
    "def case_insensitive_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Converts the input text to lowercase before tokenizing.\n",
    "    Ensures consistent tokenization for uppercase and lowercase variations.\n",
    "    \"\"\"\n",
    "    return tokenizer(text.lower())\n",
    "\n",
    "# Function to extract contextual embedding for a specific keyword using a sliding window approach\n",
    "def get_contextual_embedding_sliding_window(text, keyword, max_len=512, stride=512):\n",
    "    \"\"\"\n",
    "    Extracts contextual embeddings for a specific keyword using a sliding window approach.\n",
    "    This handles large texts by breaking them into overlapping chunks to fit the model's input size.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The full article or text to process.\n",
    "    - keyword (str): The word to extract embeddings for.\n",
    "    - max_len (int): Maximum length of tokens per chunk (default is 512 for DeBERTa).\n",
    "    - stride (int): Number of overlapping tokens between chunks (default is 256).\n",
    "\n",
    "    Returns:\n",
    "    - np.array: The averaged contextual embedding for the keyword across all chunks.\n",
    "    \"\"\"\n",
    "    # Convert text and keyword to lowercase for case-insensitive processing\n",
    "    text = text.lower()\n",
    "    keyword = keyword.lower()\n",
    "\n",
    "    # Tokenize the text into overlapping chunks with padding and truncation\n",
    "    tokens = tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',  # Return tensors in PyTorch format\n",
    "        padding=True,  # Pad tokens to max_len\n",
    "        truncation=True,  # Truncate tokens exceeding max_len\n",
    "        max_length=max_len,  # Limit chunk size to max_len (512 for most models)\n",
    "        stride=stride,  # Define the overlap between chunks\n",
    "        return_overflowing_tokens=True  # Allow creation of additional chunks for long text\n",
    "    )\n",
    "    \n",
    "    embeddings = []  # List to store embeddings for the keyword from all chunks\n",
    "    \n",
    "    # Loop through each chunk of tokens\n",
    "    for i in range(tokens['input_ids'].size(0)):  \n",
    "        # Extract the specific chunk to pass into the model\n",
    "        chunk_tokens = {key: val[i].unsqueeze(0) for key, val in tokens.items() if key in ['input_ids', 'attention_mask', 'token_type_ids']}\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            output = model(**chunk_tokens)  # Forward pass through the model\n",
    "        \n",
    "        # Tokenize the keyword and retrieve its token ID\n",
    "        keyword_id = tokenizer.encode(keyword, add_special_tokens=False)[0]\n",
    "        \n",
    "        # Find the index of the keyword in the chunk\n",
    "        keyword_index = (chunk_tokens['input_ids'] == keyword_id).nonzero(as_tuple=True)\n",
    "        \n",
    "        if len(keyword_index[1]) > 0:  # If the keyword is found in the chunk\n",
    "            # Extract the embeddings for the keyword and average across its occurrences\n",
    "            keyword_embedding = output.last_hidden_state[0, keyword_index[1], :].mean(dim=0).numpy()\n",
    "            embeddings.append(keyword_embedding)  # Append the embedding for this chunk\n",
    "    \n",
    "    # Return the average embedding across all chunks if embeddings are found, else return None\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to compute semantic polarity between two sets of embeddings\n",
    "def compute_semantic_polarity(left_embeddings, right_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the semantic polarity score between Left and Right embeddings.\n",
    "    This is done by calculating the average cosine distance between the two sets of embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - left_embeddings (list): List of embeddings for Left leaning articles.\n",
    "    - right_embeddings (list): List of embeddings for Right leaning articles.\n",
    "\n",
    "    Returns:\n",
    "    - float: The average cosine distance (semantic polarity) between the two sets of embeddings.\n",
    "    \n",
    "    How it works:\n",
    "    - Every embedding from Left articles is compared with every embedding from Right articles for a given year.\n",
    "    - The total number of pairwise comparisons is m × n, where m is the number of Left embeddings and n is the number of Fox embeddings.\n",
    "    - This ensures that all possible semantic relationships are captured.\n",
    "    \"\"\"\n",
    "    total_distance = 0  # Initialize the sum of cosine distances\n",
    "    count = 0  # Counter for the number of embedding pairs\n",
    "    \n",
    "    for left_emb in left_embeddings:\n",
    "        for right_emb in right_embeddings:\n",
    "            # Compute cosine similarity and convert it to distance (1 - similarity)\n",
    "            distance = 1 - cosine_similarity(left_emb.reshape(1, -1), right_emb.reshape(1, -1))[0][0]\n",
    "            total_distance += distance\n",
    "            count += 1  # Increment the count for each pair\n",
    "    \n",
    "    # Return the average distance if there are valid pairs, else return 0\n",
    "    return total_distance / count if count > 0 else 0\n",
    "\n",
    "# Analyze semantic polarity over time\n",
    "df['year'] = pd.to_datetime(df['date']).dt.year  # Extract the year from the 'date' column\n",
    "results = []  # List to store results for each year\n",
    "\n",
    "# Group articles by year and analyze\n",
    "for year, group in df.groupby('year'):  # Iterate through each year group\n",
    "    left_group = group[group['leaning'] == 'Left']  # Filter Left articles for the current year\n",
    "    right_group = group[group['leaning'] == 'Right']  # Filter Right articles for the current year\n",
    "    \n",
    "    # Extract embeddings for 'abortion' using the sliding window function\n",
    "    left_embeddings = [get_contextual_embedding_sliding_window(body, 'abortion') for body in left_group['body'] if get_contextual_embedding_sliding_window(body, 'abortion') is not None]\n",
    "    right_embeddings = [get_contextual_embedding_sliding_window(body, 'abortion') for body in right_group['body'] if get_contextual_embedding_sliding_window(body, 'abortion') is not None]\n",
    "    \n",
    "    # Compute the semantic polarity score for the current year\n",
    "    sp_score = compute_semantic_polarity(left_embeddings, right_embeddings)\n",
    "    results.append({'year': year, 'semantic_polarity': sp_score})  # Store the results\n",
    "\n",
    "# Convert the results to a DataFrame for easy visualization and analysis\n",
    "sp_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(sp_df)  # Output the semantic polarity scores for each year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559dd355-d645-47b9-b11c-ca7fb9d2f43d",
   "metadata": {},
   "source": [
    "### subsetting and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e46e80e5-d7ce-4071-ab9d-c680a1bf1c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>words</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "      <th>leaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32303732</td>\n",
       "      <td>933.0</td>\n",
       "      <td>20-11-04</td>\n",
       "      <td>US</td>\n",
       "      <td>buzzfeednews.com</td>\n",
       "      <td>https://www.buzzfeednews.com/article/carolinek...</td>\n",
       "      <td>this woman learned her birthmark is a sign she...</td>\n",
       "      <td>twins often have an inseparable bond, but muhl...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32303442</td>\n",
       "      <td>956.0</td>\n",
       "      <td>20-11-04</td>\n",
       "      <td>US</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>https://www.foxnews.com/us/dc-activists-blm-pr...</td>\n",
       "      <td>protests erupt in philadelphia related to elec...</td>\n",
       "      <td>the count every vote movement, which aims to e...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87507637</td>\n",
       "      <td>945.0</td>\n",
       "      <td>21-07-01</td>\n",
       "      <td>US</td>\n",
       "      <td>The Boston Globe</td>\n",
       "      <td>https://www.bostonglobe.com/2021/07/01/arts/ba...</td>\n",
       "      <td>battling the patriarchy's censor in `the man w...</td>\n",
       "      <td>it seems incredible now, when websites display...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87504480</td>\n",
       "      <td>949.0</td>\n",
       "      <td>21-07-01</td>\n",
       "      <td>US</td>\n",
       "      <td>Townhall</td>\n",
       "      <td>https://townhall.com/columnists/jordanbrittain...</td>\n",
       "      <td>the christian faithful are rising up to fight ...</td>\n",
       "      <td>this week, the united states conference of cat...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94553862</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>22-11-01</td>\n",
       "      <td>US</td>\n",
       "      <td>theatlantic.com</td>\n",
       "      <td>https://www.theatlantic.com/ideas/archive/2022...</td>\n",
       "      <td>yes, elections have consequences</td>\n",
       "      <td>americans reputedly have short attention spans...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>90146792</td>\n",
       "      <td>2531.0</td>\n",
       "      <td>22-11-01</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>https://www.foxnews.com/politics/fox-news-powe...</td>\n",
       "      <td>fox news power rankings: republicans expected ...</td>\n",
       "      <td>republicans are winning on the economy and cri...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>91719925</td>\n",
       "      <td>723.0</td>\n",
       "      <td>23-11-02</td>\n",
       "      <td>US</td>\n",
       "      <td>The Boston Globe</td>\n",
       "      <td>https://www.bostonglobe.com/2023/11/02/metro/d...</td>\n",
       "      <td>this band plans to drown out the men's march a...</td>\n",
       "      <td>kirk israel, an activist musician and member o...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>91721731</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>23-11-03</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>https://www.foxnews.com/politics/virginias-ele...</td>\n",
       "      <td>virginia's elections a key 2024 barometer and ...</td>\n",
       "      <td>youngkin wore a similar red vest two years ago...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>107130862</td>\n",
       "      <td>3219.0</td>\n",
       "      <td>24-04-01</td>\n",
       "      <td>US</td>\n",
       "      <td>theatlantic.com</td>\n",
       "      <td>https://www.theatlantic.com/politics/archive/2...</td>\n",
       "      <td>ro khanna wants to be the future of the democr...</td>\n",
       "      <td>in january, as the 2024 primary season got und...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200353125</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>24-04-01</td>\n",
       "      <td>US</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>https://www.foxnews.com/lifestyle/easter-bunny...</td>\n",
       "      <td>easter bunny not just a 'silly, secular rabbit...</td>\n",
       "      <td>if you doubt that, try to recall an ad for eas...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      textID   words      date country            source  \\\n",
       "0   32303732   933.0  20-11-04      US  buzzfeednews.com   \n",
       "1   32303442   956.0  20-11-04      US       foxnews.com   \n",
       "2   87507637   945.0  21-07-01      US  The Boston Globe   \n",
       "3   87504480   949.0  21-07-01      US          Townhall   \n",
       "4   94553862  1625.0  22-11-01      US   theatlantic.com   \n",
       "5   90146792  2531.0  22-11-01      US          Fox News   \n",
       "6   91719925   723.0  23-11-02      US  The Boston Globe   \n",
       "7   91721731  1248.0  23-11-03      US          Fox News   \n",
       "8  107130862  3219.0  24-04-01      US   theatlantic.com   \n",
       "9  200353125  1003.0  24-04-01      US       foxnews.com   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.buzzfeednews.com/article/carolinek...   \n",
       "1  https://www.foxnews.com/us/dc-activists-blm-pr...   \n",
       "2  https://www.bostonglobe.com/2021/07/01/arts/ba...   \n",
       "3  https://townhall.com/columnists/jordanbrittain...   \n",
       "4  https://www.theatlantic.com/ideas/archive/2022...   \n",
       "5  https://www.foxnews.com/politics/fox-news-powe...   \n",
       "6  https://www.bostonglobe.com/2023/11/02/metro/d...   \n",
       "7  https://www.foxnews.com/politics/virginias-ele...   \n",
       "8  https://www.theatlantic.com/politics/archive/2...   \n",
       "9  https://www.foxnews.com/lifestyle/easter-bunny...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  this woman learned her birthmark is a sign she...   \n",
       "1  protests erupt in philadelphia related to elec...   \n",
       "2  battling the patriarchy's censor in `the man w...   \n",
       "3  the christian faithful are rising up to fight ...   \n",
       "4                   yes, elections have consequences   \n",
       "5  fox news power rankings: republicans expected ...   \n",
       "6  this band plans to drown out the men's march a...   \n",
       "7  virginia's elections a key 2024 barometer and ...   \n",
       "8  ro khanna wants to be the future of the democr...   \n",
       "9  easter bunny not just a 'silly, secular rabbit...   \n",
       "\n",
       "                                                body leaning  \n",
       "0  twins often have an inseparable bond, but muhl...    Left  \n",
       "1  the count every vote movement, which aims to e...   Right  \n",
       "2  it seems incredible now, when websites display...    Left  \n",
       "3  this week, the united states conference of cat...   Right  \n",
       "4  americans reputedly have short attention spans...    Left  \n",
       "5  republicans are winning on the economy and cri...   Right  \n",
       "6  kirk israel, an activist musician and member o...    Left  \n",
       "7  youngkin wore a similar red vest two years ago...   Right  \n",
       "8  in january, as the 2024 primary season got und...    Left  \n",
       "9  if you doubt that, try to recall an ad for eas...   Right  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define years and leanings to filter\n",
    "years = ['20', '21', '22', '23', '24']\n",
    "leanings = ['Left', 'Right']\n",
    "\n",
    "# Select rows matching the condition: year starts with '20', '21', etc., and one per leaning\n",
    "final_df = pd.concat([\n",
    "    df.loc[df['date'].str.startswith(year) & (df['leaning'] == leaning)].head(1)\n",
    "    for year in years for leaning in leanings\n",
    "])\n",
    "\n",
    "# Reset index for clean output\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "# Display the result\n",
    "final_df\n",
    "\n",
    "df = final_df\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7158ff8-d1ff-4ca8-bb80-197d935b829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  semantic_polarity\n",
      "0  2021           0.606488\n",
      "1  2022           0.221814\n"
     ]
    }
   ],
   "source": [
    "# trying with BERT for the sample\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the case-insensitive tokenizer function\n",
    "def case_insensitive_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Converts the input text to lowercase before tokenizing.\n",
    "    Ensures consistent tokenization for uppercase and lowercase variations.\n",
    "    \"\"\"\n",
    "    return tokenizer(text.lower())\n",
    "\n",
    "# Function to extract contextual embedding for a specific keyword using a sliding window approach\n",
    "def get_contextual_embedding_sliding_window(text, keyword, max_len=512, stride=512):\n",
    "    \"\"\"\n",
    "    Extracts contextual embeddings for a specific keyword using a sliding window approach.\n",
    "    Handles BERT subword tokenization by matching keyword subword tokens.\n",
    "    \"\"\"\n",
    "    # Convert text and keyword to lowercase for case-insensitive processing\n",
    "    text = text.lower()\n",
    "    keyword = keyword.lower()\n",
    "    \n",
    "    # Tokenize the keyword into subwords to handle BERT's tokenization\n",
    "    keyword_ids = tokenizer.encode(keyword, add_special_tokens=False)\n",
    "\n",
    "    # Tokenize the text into overlapping chunks\n",
    "    tokens = tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "    \n",
    "    embeddings = []  # Store embeddings for the keyword\n",
    "    \n",
    "    # Loop through each chunk of tokens\n",
    "    for i in range(tokens['input_ids'].size(0)):\n",
    "        chunk_tokens = {key: val[i].unsqueeze(0) for key, val in tokens.items() if key in ['input_ids', 'attention_mask']}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(**chunk_tokens)  # Forward pass\n",
    "        \n",
    "        # Find the indices of the first keyword token in the chunk\n",
    "        input_ids = chunk_tokens['input_ids'][0].tolist()\n",
    "        for idx in range(len(input_ids) - len(keyword_ids) + 1):\n",
    "            if input_ids[idx:idx + len(keyword_ids)] == keyword_ids:\n",
    "                # Extract and average embeddings for keyword occurrences\n",
    "                keyword_embedding = output.last_hidden_state[0, idx:idx + len(keyword_ids), :].mean(dim=0).numpy()\n",
    "                embeddings.append(keyword_embedding)\n",
    "    \n",
    "    # Return the average embedding across all chunks\n",
    "    return np.mean(embeddings, axis=0) if len(embeddings) > 0 else None\n",
    "\n",
    "# Function to compute semantic polarity between two sets of embeddings\n",
    "def compute_semantic_polarity(left_embeddings, right_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the semantic polarity score between Left and Right embeddings.\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "    \n",
    "    for left_emb in left_embeddings:\n",
    "        for right_emb in right_embeddings:\n",
    "            distance = 1 - cosine_similarity(left_emb.reshape(1, -1), right_emb.reshape(1, -1))[0][0]\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    \n",
    "    return total_distance / count if count > 0 else 0\n",
    "\n",
    "# Example DataFrame (replace this with your actual data)\n",
    "# df = pd.read_csv('your_data_file.csv')  # Replace with actual data loading\n",
    "data = {\n",
    "    'date': ['21-01-01', '21-02-01', '22-01-01', '22-02-01'],\n",
    "    'leaning': ['Left', 'Right', 'Left', 'Right'],\n",
    "    'body': [\n",
    "        'Abortion rights are under attack in some states.',\n",
    "        'The abortion debate continues across the country.',\n",
    "        'Pro-choice activists fight for abortion rights.',\n",
    "        'Many citizens oppose abortion on moral grounds.'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Correctly parse the 'date' column with the specified format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%y-%m-%d', errors='coerce')  # Specify the format 'yy-mm-dd'\n",
    "\n",
    "# Drop rows where the date couldn't be parsed\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Extract the year for yearly analysis\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Group articles by year and analyze semantic polarity\n",
    "for year, group in df.groupby('year'):\n",
    "    left_group = group[group['leaning'] == 'Left']  # Filter Left-leaning articles\n",
    "    right_group = group[group['leaning'] == 'Right']  # Filter Right-leaning articles\n",
    "    \n",
    "    # Extract embeddings for the keyword 'abortion'\n",
    "    left_embeddings = [get_contextual_embedding_sliding_window(body, 'abortion') \n",
    "                       for body in left_group['body'] if get_contextual_embedding_sliding_window(body, 'abortion') is not None]\n",
    "    right_embeddings = [get_contextual_embedding_sliding_window(body, 'abortion') \n",
    "                        for body in right_group['body'] if get_contextual_embedding_sliding_window(body, 'abortion') is not None]\n",
    "    \n",
    "    # Compute semantic polarity for the year\n",
    "    sp_score = compute_semantic_polarity(left_embeddings, right_embeddings)\n",
    "    results.append({'year': year, 'semantic_polarity': sp_score})\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "sp_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(sp_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8e6cbcd-412b-4e6a-b7a6-664dc1537e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  semantic_polarity\n",
      "0  2020           0.167705\n",
      "1  2021           0.155487\n",
      "2  2022           0.159048\n",
      "3  2023           0.165120\n",
      "4  2024           0.171671\n"
     ]
    }
   ],
   "source": [
    "# maybe fire - yes fire! Years\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DebertaV2Tokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "model = AutoModel.from_pretrained('microsoft/deberta-v3-large')\n",
    "\n",
    "# Define the case-insensitive tokenizer function\n",
    "def case_insensitive_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Converts the input text to lowercase before tokenizing.\n",
    "    Ensures consistent tokenization for uppercase and lowercase variations.\n",
    "    \"\"\"\n",
    "    return tokenizer(text.lower())\n",
    "\n",
    "# Function to extract contextual embedding for a specific keyword using a sliding window approach\n",
    "def get_contextual_embedding_sliding_window(text, keyword, max_len=512, stride=256):\n",
    "    \"\"\"\n",
    "    Extracts contextual embeddings for a specific keyword using a sliding window approach.\n",
    "    \"\"\"\n",
    "    # Convert text and keyword to lowercase for case-insensitive processing\n",
    "    text = text.lower()\n",
    "    keyword = keyword.lower()\n",
    "\n",
    "    # Tokenize the text into overlapping chunks\n",
    "    tokens = tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "    \n",
    "    embeddings = []  # Store embeddings for the keyword\n",
    "    \n",
    "    # Loop through each chunk of tokens\n",
    "    for i in range(tokens['input_ids'].size(0)):\n",
    "        chunk_tokens = {key: val[i].unsqueeze(0) for key, val in tokens.items() if key in ['input_ids', 'attention_mask']}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(**chunk_tokens)  # Forward pass\n",
    "        \n",
    "        # Retrieve keyword token ID\n",
    "        keyword_id = tokenizer.encode(keyword, add_special_tokens=False)[0]\n",
    "        \n",
    "        # Find the index of the keyword\n",
    "        keyword_index = (chunk_tokens['input_ids'] == keyword_id).nonzero(as_tuple=True)\n",
    "        \n",
    "        if len(keyword_index[1]) > 0:\n",
    "            # Extract and average embeddings for keyword occurrences\n",
    "            keyword_embedding = output.last_hidden_state[0, keyword_index[1], :].mean(dim=0).numpy()\n",
    "            embeddings.append(keyword_embedding)\n",
    "    \n",
    "    # Return the average embedding across all chunks\n",
    "    return np.mean(embeddings, axis=0) if len(embeddings) > 0 else None\n",
    "\n",
    "# Function to compute semantic polarity between two sets of embeddings\n",
    "def compute_semantic_polarity(left_embeddings, right_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the semantic polarity score between Left and Right embeddings.\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "    \n",
    "    for left_emb in left_embeddings:\n",
    "        for right_emb in right_embeddings:\n",
    "            distance = 1 - cosine_similarity(left_emb.reshape(1, -1), right_emb.reshape(1, -1))[0][0]\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    \n",
    "    return total_distance / count if count > 0 else 0\n",
    "\n",
    "# Correctly parse the 'date' column with the specified format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%y-%m-%d', errors='coerce')  # Specify the format 'yy-mm-dd'\n",
    "\n",
    "# Drop rows where the date couldn't be parsed\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Extract the year for yearly analysis\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Group articles by year and analyze semantic polarity\n",
    "for year, group in df.groupby('year'):\n",
    "    left_group = group[group['leaning'] == 'Left']  # Filter Left-leaning articles\n",
    "    right_group = group[group['leaning'] == 'Right']  # Filter Right-leaning articles\n",
    "    \n",
    "    # Extract embeddings for the keyword 'abortion'\n",
    "    left_embeddings = [get_contextual_embedding_sliding_window(body, 'abortion') \n",
    "                       for body in left_group['body'] if get_contextual_embedding_sliding_window(body, 'abortion') is not None]\n",
    "    right_embeddings = [get_contextual_embedding_sliding_window(body, 'abortion') \n",
    "                        for body in right_group['body'] if get_contextual_embedding_sliding_window(body, 'abortion') is not None]\n",
    "    \n",
    "    # Compute semantic polarity for the year\n",
    "    sp_score = compute_semantic_polarity(left_embeddings, right_embeddings)\n",
    "    results.append({'year': year, 'semantic_polarity': sp_score})\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "sp_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(sp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ed20985-872a-4b3b-99d3-967fd4d191aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path where you want to save the CSV🔥\n",
    "output_file_path = \"/work/Bachelor/results_for_plots/sp_df_stride256.csv\"\n",
    "\n",
    "# Save the filtered DataFrame as a CSV file\n",
    "sp_df.to_csv(output_file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73728f-a3a9-465d-aac7-d8b48b5d018f",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f098bf4-567e-4084-9bd2-925f41d01c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  semantic_polarity\n",
      "0   2020-11           0.164017\n",
      "1   2020-12           0.000000\n",
      "2   2021-01           0.250338\n",
      "3   2021-02           0.156182\n",
      "4   2021-03           0.000000\n",
      "5   2021-04           0.000000\n",
      "6   2021-05           0.000000\n",
      "7   2021-06           0.226804\n",
      "8   2021-07           0.161126\n",
      "9   2021-08           0.000000\n",
      "10  2021-09           0.195129\n",
      "11  2021-10           0.232817\n",
      "12  2021-11           0.171114\n",
      "13  2021-12           0.181859\n",
      "14  2022-01           0.186426\n",
      "15  2022-02           0.230075\n",
      "16  2022-03           0.193148\n",
      "17  2022-04           0.159160\n",
      "18  2022-05           0.182965\n",
      "19  2022-06           0.199873\n",
      "20  2022-07           0.180288\n",
      "21  2022-08           0.161934\n",
      "22  2022-09           0.133655\n",
      "23  2022-10           0.199120\n",
      "24  2022-11           0.184042\n",
      "25  2022-12           0.247309\n",
      "26  2023-01           0.335971\n",
      "27  2023-02           0.000000\n",
      "28  2023-03           0.000000\n",
      "29  2023-04           0.168185\n",
      "30  2023-05           0.297356\n",
      "31  2023-06           0.232209\n",
      "32  2023-07           0.000000\n",
      "33  2023-08           0.138926\n",
      "34  2023-09           0.000000\n",
      "35  2023-10           0.069445\n",
      "36  2023-11           0.146890\n",
      "37  2023-12           0.000000\n",
      "38  2024-01           0.190121\n",
      "39  2024-02           0.153348\n",
      "40  2024-03           0.238891\n",
      "41  2024-04           0.000000\n",
      "42  2024-05           0.000000\n",
      "43  2024-06           0.000000\n",
      "44  2024-07           0.000000\n",
      "45  2024-08           0.000000\n",
      "46  2024-09           0.000000\n",
      "47  2024-10           0.000000\n",
      "48  2024-11           0.000000\n"
     ]
    }
   ],
   "source": [
    "### 🔥🔥🔥🔥🔥🔥🔥\n",
    "# maybe fire for months - yes fire!!!!!!!\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DebertaV2Tokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "model = AutoModel.from_pretrained('microsoft/deberta-v3-large')\n",
    "\n",
    "# Define the case-insensitive tokenizer function\n",
    "def case_insensitive_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Converts the input text to lowercase before tokenizing.\n",
    "    Ensures consistent tokenization for uppercase and lowercase variations.\n",
    "    \"\"\"\n",
    "    return tokenizer(text.lower())\n",
    "\n",
    "# Function to extract contextual embedding for a specific keyword using a sliding window approach (no didn't do that)\n",
    "def get_contextual_embedding_sliding_window(text, keyword, max_len=512, stride=512):\n",
    "    \"\"\"\n",
    "    Extracts contextual embeddings for a specific keyword using a sliding window approach.\n",
    "    \"\"\"\n",
    "    # Convert text and keyword to lowercase for case-insensitive processing\n",
    "    text = text.lower()\n",
    "    keyword = keyword.lower()\n",
    "\n",
    "    # Tokenize the text into overlapping chunks\n",
    "    tokens = tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "    \n",
    "    embeddings = []  # Store embeddings for the keyword\n",
    "    \n",
    "    # Loop through each chunk of tokens\n",
    "    for i in range(tokens['input_ids'].size(0)):\n",
    "        chunk_tokens = {key: val[i].unsqueeze(0) for key, val in tokens.items() if key in ['input_ids', 'attention_mask']}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(**chunk_tokens)  # Forward pass\n",
    "        \n",
    "        # Retrieve keyword token ID\n",
    "        keyword_id = tokenizer.encode(keyword, add_special_tokens=False)[0]\n",
    "        \n",
    "        # Find the index of the keyword\n",
    "        keyword_index = (chunk_tokens['input_ids'] == keyword_id).nonzero(as_tuple=True)\n",
    "        \n",
    "        if len(keyword_index[1]) > 0:\n",
    "            # Extract and average embeddings for keyword occurrences\n",
    "            keyword_embedding = output.last_hidden_state[0, keyword_index[1], :].mean(dim=0).numpy()\n",
    "            embeddings.append(keyword_embedding)\n",
    "    \n",
    "    # Return the average embedding across all chunks\n",
    "    return np.mean(embeddings, axis=0) if len(embeddings) > 0 else None\n",
    "\n",
    "# Function to compute semantic polarity between two sets of embeddings\n",
    "def compute_semantic_polarity(left_embeddings, right_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the semantic polarity score between Left and Right embeddings.\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "    \n",
    "    for left_emb in left_embeddings:\n",
    "        for right_emb in right_embeddings:\n",
    "            distance = 1 - cosine_similarity(left_emb.reshape(1, -1), right_emb.reshape(1, -1))[0][0]\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    \n",
    "    return total_distance / count if count > 0 else 0\n",
    "\n",
    "# Correctly parse the 'date' column with the specified format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%y-%m-%d', errors='coerce')  # Specify the format 'yy-mm-dd'\n",
    "\n",
    "# Drop rows where the date couldn't be parsed\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Extract the year-month for monthly analysis\n",
    "df['year_month'] = df['date'].dt.to_period('M')  # Creates 'YYYY-MM' format\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Group articles by month and analyze semantic polarity\n",
    "for month, group in df.groupby('year_month'):\n",
    "    left_group = group[group['leaning'] == 'Left']  # Filter Left-leaning articles\n",
    "    right_group = group[group['leaning'] == 'Right']  # Filter Right-leaning articles\n",
    "    \n",
    "    # Extract embeddings for the keyword 'abortion'\n",
    "    left_embeddings = [get_contextual_embedding_sliding_window(body, 'protesters') \n",
    "                       for body in left_group['body'] if get_contextual_embedding_sliding_window(body, 'protesters') is not None]\n",
    "    right_embeddings = [get_contextual_embedding_sliding_window(body, 'protesters') \n",
    "                        for body in right_group['body'] if get_contextual_embedding_sliding_window(body, 'protesters') is not None]\n",
    "    \n",
    "    # Compute semantic polarity for the month\n",
    "    sp_score = compute_semantic_polarity(left_embeddings, right_embeddings)\n",
    "    results.append({'month': month.strftime('%Y-%m'), 'semantic_polarity': sp_score})\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "sp_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(sp_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b6937b4-f3f6-484f-b7db-b7756ec4757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path where you want to save the CSV🔥\n",
    "output_file_path = \"/work/Bachelor/results_for_plots/protesters.csv\"\n",
    "\n",
    "# Save the filtered DataFrame as a CSV file\n",
    "sp_df.to_csv(output_file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2d346-e656-4c3a-a395-6abf974d983b",
   "metadata": {},
   "source": [
    "#### multi token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5762d566-1393-45d2-b21c-2939bebe95ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Preprocess the 'body' column in your dataframe\n",
    "def standardize_roe_variants(text):\n",
    "    \"\"\"\n",
    "    Replaces variations of 'roe v. wade', 'roe vs. wade', etc. with 'roe v wade'.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'roe\\s+v[.\\s]*wade', 'roe v wade', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'roe\\s+vs[.\\s]*wade', 'roe v wade', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'body' column\n",
    "df['body'] = df['body'].apply(standardize_roe_variants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7156b68e-b1c9-451b-b068-a129a7ea30d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 106\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Extract embeddings for the keyword\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     left_embeddings \u001b[38;5;241m=\u001b[39m [get_contextual_embedding_sliding_window(body, keyword) \n\u001b[1;32m    104\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m body \u001b[38;5;129;01min\u001b[39;00m left_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m get_contextual_embedding_sliding_window(body, keyword) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    105\u001b[0m     right_embeddings \u001b[38;5;241m=\u001b[39m [get_contextual_embedding_sliding_window(body, keyword) \n\u001b[0;32m--> 106\u001b[0m                         \u001b[38;5;28;01mfor\u001b[39;00m body \u001b[38;5;129;01min\u001b[39;00m right_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mget_contextual_embedding_sliding_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Compute semantic polarity for the month\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m left_embeddings \u001b[38;5;129;01mand\u001b[39;00m right_embeddings:\n",
      "Cell \u001b[0;32mIn[36], line 51\u001b[0m, in \u001b[0;36mget_contextual_embedding_sliding_window\u001b[0;34m(text, keyword, max_len, stride)\u001b[0m\n\u001b[1;32m     48\u001b[0m chunk_tokens \u001b[38;5;241m=\u001b[39m {key: val[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 51\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchunk_tokens\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Find the indices of the first occurrence of the phrase (sequence of token IDs)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m chunk_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:871\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    861\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    863\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    864\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    865\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    869\u001b[0m )\n\u001b[0;32m--> 871\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    878\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:675\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    665\u001b[0m     output_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    666\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    667\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    672\u001b[0m         output_attentions,\n\u001b[1;32m    673\u001b[0m     )\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 675\u001b[0m     output_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    685\u001b[0m     all_attentions \u001b[38;5;241m=\u001b[39m all_attentions \u001b[38;5;241m+\u001b[39m (attn_weights,)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:443\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    436\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m--> 443\u001b[0m     attention_output, att_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m    452\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:376\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    369\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    375\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m--> 376\u001b[0m     self_output, att_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m         query_states \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:242\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    240\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_proj(query_states), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads)\n\u001b[1;32m    241\u001b[0m key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_proj(hidden_states), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads)\n\u001b[0;32m--> 242\u001b[0m value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads)\n\u001b[1;32m    244\u001b[0m rel_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# chat went wild - maybe this runs multiple keywords at once...\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DebertaV2Tokenizer, AutoModel\n",
    "import re\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "model = AutoModel.from_pretrained('microsoft/deberta-v3-large')\n",
    "\n",
    "# Preprocess 'body' column to standardize 'roe v wade' variations\n",
    "def standardize_roe_variants(text):\n",
    "    \"\"\"\n",
    "    Replaces variations of 'roe v. wade', 'roe vs. wade', etc. with 'roe v wade'.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'roe\\s+v[.\\s]*wade', 'roe v wade', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'roe\\s+vs[.\\s]*wade', 'roe v wade', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# Function to extract contextual embedding for a specific keyword or multi-word phrase\n",
    "def get_contextual_embedding_sliding_window(text, keyword, max_len=512, stride=512):\n",
    "    \"\"\"\n",
    "    Extracts contextual embeddings for a specific keyword or multi-word phrase using a sliding window approach.\n",
    "    \"\"\"\n",
    "    # Convert text and keyword to lowercase for case-insensitive processing\n",
    "    text = text.lower()\n",
    "    keyword = keyword.lower()\n",
    "    \n",
    "    # Tokenize the keyword into subwords to handle multi-word phrases\n",
    "    keyword_ids = tokenizer.encode(keyword, add_special_tokens=False)\n",
    "\n",
    "    # Tokenize the text into overlapping chunks\n",
    "    tokens = tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "    \n",
    "    embeddings = []  # Store embeddings for the keyword\n",
    "    \n",
    "    # Loop through each chunk of tokens\n",
    "    for i in range(tokens['input_ids'].size(0)):\n",
    "        chunk_tokens = {key: val[i].unsqueeze(0) for key, val in tokens.items() if key in ['input_ids', 'attention_mask']}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(**chunk_tokens)  # Forward pass\n",
    "        \n",
    "        # Find the indices of the first occurrence of the phrase (sequence of token IDs)\n",
    "        input_ids = chunk_tokens['input_ids'][0].tolist()\n",
    "        for idx in range(len(input_ids) - len(keyword_ids) + 1):\n",
    "            if input_ids[idx:idx + len(keyword_ids)] == keyword_ids:\n",
    "                # Extract and average embeddings for the entire phrase\n",
    "                keyword_embedding = output.last_hidden_state[0, idx:idx + len(keyword_ids), :].mean(dim=0).numpy()\n",
    "                embeddings.append(keyword_embedding)\n",
    "    \n",
    "    # Return the average embedding across all chunks\n",
    "    return np.mean(embeddings, axis=0) if len(embeddings) > 0 else None\n",
    "\n",
    "# Function to compute semantic polarity between two sets of embeddings\n",
    "def compute_semantic_polarity(left_embeddings, right_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the semantic polarity score between Left and Right embeddings.\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "    \n",
    "    for left_emb in left_embeddings:\n",
    "        for right_emb in right_embeddings:\n",
    "            distance = 1 - cosine_similarity(left_emb.reshape(1, -1), right_emb.reshape(1, -1))[0][0]\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    \n",
    "    return total_distance / count if count > 0 else 0\n",
    "\n",
    "# Load the DataFrame\n",
    "# df = pd.read_csv(\"path_to_your_file.csv\")  # Replace with your file path\n",
    "\n",
    "# Example preprocessing: Apply standardization to the 'body' column\n",
    "df['body'] = df['body'].apply(standardize_roe_variants)\n",
    "\n",
    "# Extract the year-month for monthly analysis\n",
    "df['year_month'] = pd.to_datetime(df['date'], format='%y-%m-%d', errors='coerce').dt.to_period('M')\n",
    "df = df.dropna(subset=['year_month'])  # Drop rows where 'year_month' couldn't be parsed\n",
    "\n",
    "# List of keywords/phrases to analyze\n",
    "keywords = ['roe v wade', 'abortion', 'pro-choice', 'pro-life']\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Group articles by month and analyze semantic polarity for each keyword\n",
    "for month, group in df.groupby('year_month'):\n",
    "    left_group = group[group['leaning'] == 'Left']  # Filter Left-leaning articles\n",
    "    right_group = group[group['leaning'] == 'Right']  # Filter Right-leaning articles\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        # Extract embeddings for the keyword\n",
    "        left_embeddings = [get_contextual_embedding_sliding_window(body, keyword) \n",
    "                           for body in left_group['body'] if get_contextual_embedding_sliding_window(body, keyword) is not None]\n",
    "        right_embeddings = [get_contextual_embedding_sliding_window(body, keyword) \n",
    "                            for body in right_group['body'] if get_contextual_embedding_sliding_window(body, keyword) is not None]\n",
    "        \n",
    "        # Compute semantic polarity for the month\n",
    "        if left_embeddings and right_embeddings:\n",
    "            sp_score = compute_semantic_polarity(left_embeddings, right_embeddings)\n",
    "            results.append({\n",
    "                'month': month.strftime('%Y-%m'),\n",
    "                'keyword': keyword,\n",
    "                'semantic_polarity': sp_score\n",
    "            })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "sp_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(sp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7587ab28-b2c1-4a0d-81a3-5c1490ea0985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f529cc35fb5749c8b8bcd9d49ca70748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c890691ed1aa40cea30f34744ede51ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ca948a7fb74293bc1189bb7551d9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2480f529c60401092ab5a018cd7881a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     month  semantic_polarity\n",
      "0  2022-05           0.187319\n",
      "1  2022-06           0.171925\n",
      "2  2022-07           0.155236\n",
      "3  2022-08           0.133299\n",
      "4  2022-09           0.202351\n",
      "5  2022-11           0.247092\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DebertaV2Tokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "model = AutoModel.from_pretrained('microsoft/deberta-v3-large')\n",
    "\n",
    "# Function to extract contextual embedding for a specific multi-word phrase\n",
    "def get_contextual_embedding_sliding_window(text, keyword, max_len=512, stride=512):\n",
    "    \"\"\"\n",
    "    Extracts contextual embeddings for a specific keyword or multi-word phrase using a sliding window approach.\n",
    "    \"\"\"\n",
    "    # Convert text and keyword to lowercase for case-insensitive processing\n",
    "    text = text.lower()\n",
    "    keyword = keyword.lower()\n",
    "    \n",
    "    # Tokenize the keyword into subwords to handle multi-word phrases\n",
    "    keyword_ids = tokenizer.encode(keyword, add_special_tokens=False)\n",
    "\n",
    "    # Tokenize the text into overlapping chunks\n",
    "    tokens = tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "    \n",
    "    embeddings = []  # Store embeddings for the keyword\n",
    "    \n",
    "    # Loop through each chunk of tokens\n",
    "    for i in range(tokens['input_ids'].size(0)):\n",
    "        chunk_tokens = {key: val[i].unsqueeze(0) for key, val in tokens.items() if key in ['input_ids', 'attention_mask']}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(**chunk_tokens)  # Forward pass\n",
    "        \n",
    "        # Find the indices of the first occurrence of the phrase (sequence of token IDs)\n",
    "        input_ids = chunk_tokens['input_ids'][0].tolist()\n",
    "        for idx in range(len(input_ids) - len(keyword_ids) + 1):\n",
    "            if input_ids[idx:idx + len(keyword_ids)] == keyword_ids:\n",
    "                # Extract and average embeddings for the entire phrase\n",
    "                keyword_embedding = output.last_hidden_state[0, idx:idx + len(keyword_ids), :].mean(dim=0).numpy()\n",
    "                embeddings.append(keyword_embedding)\n",
    "    \n",
    "    # Return the average embedding across all chunks\n",
    "    return np.mean(embeddings, axis=0) if len(embeddings) > 0 else None\n",
    "\n",
    "# Function to compute semantic polarity between two sets of embeddings\n",
    "def compute_semantic_polarity(left_embeddings, right_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the semantic polarity score between Left and Right embeddings.\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "    \n",
    "    for left_emb in left_embeddings:\n",
    "        for right_emb in right_embeddings:\n",
    "            distance = 1 - cosine_similarity(left_emb.reshape(1, -1), right_emb.reshape(1, -1))[0][0]\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    \n",
    "    return total_distance / count if count > 0 else 0\n",
    "\n",
    "# Extract the year-month for monthly analysis\n",
    "df['year_month'] = pd.to_datetime(df['date'], format='%y-%m-%d', errors='coerce').dt.to_period('M')\n",
    "df = df.dropna(subset=['year_month'])  # Drop rows where 'year_month' couldn't be parsed\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Group articles by month and analyze semantic polarity for \"roe v wade\"\n",
    "for month, group in df.groupby('year_month'):\n",
    "    left_group = group[group['leaning'] == 'Left']  # Filter Left-leaning articles\n",
    "    right_group = group[group['leaning'] == 'Right']  # Filter Right-leaning articles\n",
    "    \n",
    "    # Extract embeddings for \"roe v wade\"\n",
    "    left_embeddings = [get_contextual_embedding_sliding_window(body, 'roe v wade') \n",
    "                       for body in left_group['body'] if get_contextual_embedding_sliding_window(body, 'roe v wade') is not None]\n",
    "    right_embeddings = [get_contextual_embedding_sliding_window(body, 'roe v wade') \n",
    "                        for body in right_group['body'] if get_contextual_embedding_sliding_window(body, 'roe v wade') is not None]\n",
    "    \n",
    "    # Compute semantic polarity for the month\n",
    "    if left_embeddings and right_embeddings:\n",
    "        sp_score = compute_semantic_polarity(left_embeddings, right_embeddings)\n",
    "        results.append({\n",
    "            'month': month.strftime('%Y-%m'),\n",
    "            'semantic_polarity': sp_score\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "sp_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(sp_df)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "# sp_df.to_csv(\"semantic_polarity_roe_v_wade.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd071f63-a0d9-4c79-8469-bdb8999eded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path where you want to save the CSV🔥\n",
    "output_file_path = \"/work/Bachelor/results_for_plots/sp_roevwade_df.csv\"\n",
    "\n",
    "# Save the filtered DataFrame as a CSV file\n",
    "sp_df.to_csv(output_file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1f3009-b848-471b-b5fd-36b053f6c2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>words</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "      <th>leaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91719925</td>\n",
       "      <td>723.0</td>\n",
       "      <td>23-11-02</td>\n",
       "      <td>US</td>\n",
       "      <td>The Boston Globe</td>\n",
       "      <td>https://www.bostonglobe.com/2023/11/02/metro/d...</td>\n",
       "      <td>this band plans to drown out the men's march a...</td>\n",
       "      <td>kirk israel, an activist musician and member o...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91721731</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>23-11-03</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>https://www.foxnews.com/politics/virginias-ele...</td>\n",
       "      <td>virginia's elections a key 2024 barometer and ...</td>\n",
       "      <td>youngkin wore a similar red vest two years ago...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103177268</td>\n",
       "      <td>1947.0</td>\n",
       "      <td>23-11-03</td>\n",
       "      <td>US</td>\n",
       "      <td>inquisitr.com</td>\n",
       "      <td>https://www.theatlantic.com/ideas/archive/2023...</td>\n",
       "      <td>here's what biden can do to change his grim po...</td>\n",
       "      <td>whatever your theory, it should take into acco...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103177271</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>23-11-03</td>\n",
       "      <td>US</td>\n",
       "      <td>inquisitr.com</td>\n",
       "      <td>https://www.theatlantic.com/ideas/archive/2023...</td>\n",
       "      <td>don't equate anti-zionism with anti-semitism</td>\n",
       "      <td>on october 7, the islamist militant group hama...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103177279</td>\n",
       "      <td>2607.0</td>\n",
       "      <td>23-11-03</td>\n",
       "      <td>US</td>\n",
       "      <td>inquisitr.com</td>\n",
       "      <td>https://www.theatlantic.com/books/archive/2023...</td>\n",
       "      <td>do you have free will?</td>\n",
       "      <td>writing a review is an exercise in free will. ...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090</th>\n",
       "      <td>97824446</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>23-03-28</td>\n",
       "      <td>US</td>\n",
       "      <td>bostonglobe.com</td>\n",
       "      <td>https://bostonglobe.com/2023/03/26/metro/rev-e...</td>\n",
       "      <td>rev. elinor lockwood yeo, reproductive rights ...</td>\n",
       "      <td>rev. elinor lockwood yeo of newton, a reproduc...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8091</th>\n",
       "      <td>97847400</td>\n",
       "      <td>648.0</td>\n",
       "      <td>23-03-29</td>\n",
       "      <td>US</td>\n",
       "      <td>sfchronicle.com</td>\n",
       "      <td>https://sfchronicle.com/opinion/letterstotheed...</td>\n",
       "      <td>letters: how the country can respond to nashvi...</td>\n",
       "      <td>enough with the \" thoughts and prayers. \" thre...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8092</th>\n",
       "      <td>97848056</td>\n",
       "      <td>918.0</td>\n",
       "      <td>23-03-29</td>\n",
       "      <td>US</td>\n",
       "      <td>bostonglobe.com</td>\n",
       "      <td>https://bostonglobe.com/2023/03/28/opinion/abo...</td>\n",
       "      <td>the latest antiabortion tactic: asserting the ...</td>\n",
       "      <td>antiabortion activist trooper elwonger, 25, fe...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8093</th>\n",
       "      <td>90760423</td>\n",
       "      <td>1224.0</td>\n",
       "      <td>23-03-31</td>\n",
       "      <td>US</td>\n",
       "      <td>The Daily Beast</td>\n",
       "      <td>https://www.thedailybeast.com/satan-wants-you-...</td>\n",
       "      <td>inside the horrific (contested) abuse story th...</td>\n",
       "      <td>how did one discredited biography ignite one o...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8094</th>\n",
       "      <td>97893538</td>\n",
       "      <td>2247.0</td>\n",
       "      <td>23-03-31</td>\n",
       "      <td>US</td>\n",
       "      <td>theatlantic.com</td>\n",
       "      <td>https://www.theatlantic.com/politics/archive/2...</td>\n",
       "      <td>the first electoral test of trump's indictment</td>\n",
       "      <td>the most important election of 2023 may also o...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8095 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID   words      date country            source  \\\n",
       "0      91719925   723.0  23-11-02      US  The Boston Globe   \n",
       "1      91721731  1248.0  23-11-03      US          Fox News   \n",
       "2     103177268  1947.0  23-11-03      US     inquisitr.com   \n",
       "3     103177271  1513.0  23-11-03      US     inquisitr.com   \n",
       "4     103177279  2607.0  23-11-03      US     inquisitr.com   \n",
       "...         ...     ...       ...     ...               ...   \n",
       "8090   97824446  1244.0  23-03-28      US   bostonglobe.com   \n",
       "8091   97847400   648.0  23-03-29      US   sfchronicle.com   \n",
       "8092   97848056   918.0  23-03-29      US   bostonglobe.com   \n",
       "8093   90760423  1224.0  23-03-31      US   The Daily Beast   \n",
       "8094   97893538  2247.0  23-03-31      US   theatlantic.com   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://www.bostonglobe.com/2023/11/02/metro/d...   \n",
       "1     https://www.foxnews.com/politics/virginias-ele...   \n",
       "2     https://www.theatlantic.com/ideas/archive/2023...   \n",
       "3     https://www.theatlantic.com/ideas/archive/2023...   \n",
       "4     https://www.theatlantic.com/books/archive/2023...   \n",
       "...                                                 ...   \n",
       "8090  https://bostonglobe.com/2023/03/26/metro/rev-e...   \n",
       "8091  https://sfchronicle.com/opinion/letterstotheed...   \n",
       "8092  https://bostonglobe.com/2023/03/28/opinion/abo...   \n",
       "8093  https://www.thedailybeast.com/satan-wants-you-...   \n",
       "8094  https://www.theatlantic.com/politics/archive/2...   \n",
       "\n",
       "                                               headline  \\\n",
       "0     this band plans to drown out the men's march a...   \n",
       "1     virginia's elections a key 2024 barometer and ...   \n",
       "2     here's what biden can do to change his grim po...   \n",
       "3          don't equate anti-zionism with anti-semitism   \n",
       "4                                do you have free will?   \n",
       "...                                                 ...   \n",
       "8090  rev. elinor lockwood yeo, reproductive rights ...   \n",
       "8091  letters: how the country can respond to nashvi...   \n",
       "8092  the latest antiabortion tactic: asserting the ...   \n",
       "8093  inside the horrific (contested) abuse story th...   \n",
       "8094     the first electoral test of trump's indictment   \n",
       "\n",
       "                                                   body leaning  \n",
       "0     kirk israel, an activist musician and member o...    Left  \n",
       "1     youngkin wore a similar red vest two years ago...   Right  \n",
       "2     whatever your theory, it should take into acco...    Left  \n",
       "3     on october 7, the islamist militant group hama...    Left  \n",
       "4     writing a review is an exercise in free will. ...    Left  \n",
       "...                                                 ...     ...  \n",
       "8090  rev. elinor lockwood yeo of newton, a reproduc...    Left  \n",
       "8091  enough with the \" thoughts and prayers. \" thre...    Left  \n",
       "8092  antiabortion activist trooper elwonger, 25, fe...    Left  \n",
       "8093  how did one discredited biography ignite one o...    Left  \n",
       "8094  the most important election of 2023 may also o...    Left  \n",
       "\n",
       "[8095 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Replace \"pro-choice\" or \"pro choice\" with \"prochoice\"\n",
    "#df['body'] = df['body'].str.replace(r'\\bpro[-\\s]?choice\\b', 'prochoice', flags=re.IGNORECASE, regex=True)\n",
    "\n",
    "# Replace \"pro-life\" or \"pro life\" with \"prolife\"\n",
    "df['body'] = df['body'].str.replace(r'\\bpro[-\\s]?life\\b', 'prolife', flags=re.IGNORECASE, regex=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789f1f3-5a24-4b63-b76f-6252dcfb0f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a4fa96d76c49b69cbac1d7e9f693e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0109c811ac4df0bfa2d95d62e9d5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9517ae2dca0b428b806bfb44f3329016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d60bb21dc084477aeae33c61b4fd74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 🔥🔥🔥🔥🔥🔥🔥\n",
    "# maybe fire for months - yes fire!!!!!!!\n",
    "# did not do sliding window approach here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DebertaV2Tokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "model = AutoModel.from_pretrained('microsoft/deberta-v3-large')\n",
    "\n",
    "# Define the case-insensitive tokenizer function\n",
    "def case_insensitive_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Converts the input text to lowercase before tokenizing.\n",
    "    Ensures consistent tokenization for uppercase and lowercase variations.\n",
    "    \"\"\"\n",
    "    return tokenizer(text.lower())\n",
    "\n",
    "# Function to extract contextual embedding for a specific keyword using a sliding window approach\n",
    "def get_contextual_embedding_sliding_window(text, keyword, max_len=512, stride=512):\n",
    "    \"\"\"\n",
    "    Extracts contextual embeddings for a specific keyword using a sliding window approach.\n",
    "    \"\"\"\n",
    "    # Convert text and keyword to lowercase for case-insensitive processing\n",
    "    text = text.lower()\n",
    "    keyword = keyword.lower()\n",
    "\n",
    "    # Tokenize the text into overlapping chunks\n",
    "    tokens = tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "    \n",
    "    embeddings = []  # Store embeddings for the keyword\n",
    "    \n",
    "    # Loop through each chunk of tokens\n",
    "    for i in range(tokens['input_ids'].size(0)):\n",
    "        chunk_tokens = {key: val[i].unsqueeze(0) for key, val in tokens.items() if key in ['input_ids', 'attention_mask']}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(**chunk_tokens)  # Forward pass\n",
    "        \n",
    "        # Retrieve keyword token ID\n",
    "        keyword_id = tokenizer.encode(keyword, add_special_tokens=False)[0]\n",
    "        \n",
    "        # Find the index of the keyword\n",
    "        keyword_index = (chunk_tokens['input_ids'] == keyword_id).nonzero(as_tuple=True)\n",
    "        \n",
    "        if len(keyword_index[1]) > 0:\n",
    "            # Extract and average embeddings for keyword occurrences\n",
    "            keyword_embedding = output.last_hidden_state[0, keyword_index[1], :].mean(dim=0).numpy()\n",
    "            embeddings.append(keyword_embedding)\n",
    "    \n",
    "    # Return the average embedding across all chunks\n",
    "    return np.mean(embeddings, axis=0) if len(embeddings) > 0 else None\n",
    "\n",
    "# Function to compute semantic polarity between two sets of embeddings\n",
    "def compute_semantic_polarity(left_embeddings, right_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the semantic polarity score between Left and Right embeddings.\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "    \n",
    "    for left_emb in left_embeddings:\n",
    "        for right_emb in right_embeddings:\n",
    "            distance = 1 - cosine_similarity(left_emb.reshape(1, -1), right_emb.reshape(1, -1))[0][0]\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    \n",
    "    return total_distance / count if count > 0 else 0\n",
    "\n",
    "# Correctly parse the 'date' column with the specified format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%y-%m-%d', errors='coerce')  # Specify the format 'yy-mm-dd'\n",
    "\n",
    "# Drop rows where the date couldn't be parsed\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Extract the year-month for monthly analysis\n",
    "df['year_month'] = df['date'].dt.to_period('M')  # Creates 'YYYY-MM' format\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Group articles by month and analyze semantic polarity\n",
    "for month, group in df.groupby('year_month'):\n",
    "    left_group = group[group['leaning'] == 'Left']  # Filter Left-leaning articles\n",
    "    right_group = group[group['leaning'] == 'Right']  # Filter Right-leaning articles\n",
    "    \n",
    "    # Extract embeddings for the keyword 'abortion'\n",
    "    left_embeddings = [get_contextual_embedding_sliding_window(body, 'prolife') \n",
    "                       for body in left_group['body'] if get_contextual_embedding_sliding_window(body, 'prolife') is not None]\n",
    "    right_embeddings = [get_contextual_embedding_sliding_window(body, 'prolife') \n",
    "                        for body in right_group['body'] if get_contextual_embedding_sliding_window(body, 'prolife') is not None]\n",
    "    \n",
    "    # Compute semantic polarity for the month\n",
    "    sp_score = compute_semantic_polarity(left_embeddings, right_embeddings)\n",
    "    results.append({'month': month.strftime('%Y-%m'), 'semantic_polarity': sp_score})\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "sp_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(sp_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012d78b-c97b-4653-808f-756a36d88bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path where you want to save the CSV🔥\n",
    "output_file_path = \"/work/Bachelor/results_for_plots/sp_prolife3_df.csv\"\n",
    "\n",
    "# Save the filtered DataFrame as a CSV file\n",
    "sp_df.to_csv(output_file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e7d57-48e6-4c29-8bcb-4afc42db5838",
   "metadata": {},
   "source": [
    "## Without sliding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89c897f-00e6-47c3-ad6b-1ff0fd80a3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>words</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "      <th>leaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91719925</td>\n",
       "      <td>723.0</td>\n",
       "      <td>23-11-02</td>\n",
       "      <td>US</td>\n",
       "      <td>The Boston Globe</td>\n",
       "      <td>https://www.bostonglobe.com/2023/11/02/metro/d...</td>\n",
       "      <td>this band plans to drown out the men's march a...</td>\n",
       "      <td>kirk israel, an activist musician and member o...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>91734214</td>\n",
       "      <td>1651.0</td>\n",
       "      <td>23-11-06</td>\n",
       "      <td>US</td>\n",
       "      <td>The Atlantic</td>\n",
       "      <td>https://www.theatlantic.com/ideas/archive/2023...</td>\n",
       "      <td>how is child marriage still legal in the u.s.?</td>\n",
       "      <td>this past spring, as part of my work teaching ...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>91741169</td>\n",
       "      <td>276.0</td>\n",
       "      <td>23-11-08</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>https://www.foxnews.com/us/pro-life-activist-m...</td>\n",
       "      <td>pro-life activist mark houck files lawsuit aga...</td>\n",
       "      <td>a catholic activist is suing the justice depar...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>103306212</td>\n",
       "      <td>2098.0</td>\n",
       "      <td>23-11-08</td>\n",
       "      <td>US</td>\n",
       "      <td>marketwatch.com</td>\n",
       "      <td>https://www.theatlantic.com/ideas/archive/2023...</td>\n",
       "      <td>why abortion rights keep winning in red states</td>\n",
       "      <td>abortion foes thought roe v. wade'sreversal wo...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>87505877</td>\n",
       "      <td>654.0</td>\n",
       "      <td>21-07-01</td>\n",
       "      <td>US</td>\n",
       "      <td>CNSNews.com</td>\n",
       "      <td>https://cnsnews.com/article/washington/ashlian...</td>\n",
       "      <td>is a 15-week-old unborn baby a human being? se...</td>\n",
       "      <td>( cns news ) -- when asked if a 15-week-old un...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8046</th>\n",
       "      <td>97525942</td>\n",
       "      <td>1495.0</td>\n",
       "      <td>23-03-14</td>\n",
       "      <td>US</td>\n",
       "      <td>bostonglobe.com</td>\n",
       "      <td>https://bostonglobe.com/2023/03/13/metro/patri...</td>\n",
       "      <td>patricia schroeder, congresswoman who wielded ...</td>\n",
       "      <td>u.s. rep. pat schroeder, d-colo., sat on the p...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8072</th>\n",
       "      <td>90728356</td>\n",
       "      <td>463.0</td>\n",
       "      <td>23-03-21</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>https://www.foxnews.com/politics/smuggling-abo...</td>\n",
       "      <td>smuggling the abortion pill into texas could l...</td>\n",
       "      <td>lawyers who filed a lawsuit challenging the fd...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8073</th>\n",
       "      <td>97697350</td>\n",
       "      <td>1793.0</td>\n",
       "      <td>23-03-22</td>\n",
       "      <td>US</td>\n",
       "      <td>theatlantic.com</td>\n",
       "      <td>https://www.theatlantic.com/ideas/archive/2023...</td>\n",
       "      <td>the malthusians are back</td>\n",
       "      <td>scolding regular people for contributing to cl...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8084</th>\n",
       "      <td>90739332</td>\n",
       "      <td>529.0</td>\n",
       "      <td>23-03-25</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>https://www.foxnews.com/politics/dems-want-fun...</td>\n",
       "      <td>dems want to fund abortion `everywhere' in the...</td>\n",
       "      <td>judicial crisis network president carrie sever...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8085</th>\n",
       "      <td>97763074</td>\n",
       "      <td>926.0</td>\n",
       "      <td>23-03-25</td>\n",
       "      <td>US</td>\n",
       "      <td>bostonglobe.com</td>\n",
       "      <td>https://bostonglobe.com/2023/03/24/metro/abort...</td>\n",
       "      <td>abortion bills face uphill battle in n.h. senate</td>\n",
       "      <td>new hampshire house of representatives voting ...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1020 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID   words      date country            source  \\\n",
       "0      91719925   723.0  23-11-02      US  The Boston Globe   \n",
       "12     91734214  1651.0  23-11-06      US      The Atlantic   \n",
       "34     91741169   276.0  23-11-08      US          Fox News   \n",
       "40    103306212  2098.0  23-11-08      US   marketwatch.com   \n",
       "83     87505877   654.0  21-07-01      US       CNSNews.com   \n",
       "...         ...     ...       ...     ...               ...   \n",
       "8046   97525942  1495.0  23-03-14      US   bostonglobe.com   \n",
       "8072   90728356   463.0  23-03-21      US          Fox News   \n",
       "8073   97697350  1793.0  23-03-22      US   theatlantic.com   \n",
       "8084   90739332   529.0  23-03-25      US          Fox News   \n",
       "8085   97763074   926.0  23-03-25      US   bostonglobe.com   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://www.bostonglobe.com/2023/11/02/metro/d...   \n",
       "12    https://www.theatlantic.com/ideas/archive/2023...   \n",
       "34    https://www.foxnews.com/us/pro-life-activist-m...   \n",
       "40    https://www.theatlantic.com/ideas/archive/2023...   \n",
       "83    https://cnsnews.com/article/washington/ashlian...   \n",
       "...                                                 ...   \n",
       "8046  https://bostonglobe.com/2023/03/13/metro/patri...   \n",
       "8072  https://www.foxnews.com/politics/smuggling-abo...   \n",
       "8073  https://www.theatlantic.com/ideas/archive/2023...   \n",
       "8084  https://www.foxnews.com/politics/dems-want-fun...   \n",
       "8085  https://bostonglobe.com/2023/03/24/metro/abort...   \n",
       "\n",
       "                                               headline  \\\n",
       "0     this band plans to drown out the men's march a...   \n",
       "12       how is child marriage still legal in the u.s.?   \n",
       "34    pro-life activist mark houck files lawsuit aga...   \n",
       "40       why abortion rights keep winning in red states   \n",
       "83    is a 15-week-old unborn baby a human being? se...   \n",
       "...                                                 ...   \n",
       "8046  patricia schroeder, congresswoman who wielded ...   \n",
       "8072  smuggling the abortion pill into texas could l...   \n",
       "8073                           the malthusians are back   \n",
       "8084  dems want to fund abortion `everywhere' in the...   \n",
       "8085   abortion bills face uphill battle in n.h. senate   \n",
       "\n",
       "                                                   body leaning  \n",
       "0     kirk israel, an activist musician and member o...    Left  \n",
       "12    this past spring, as part of my work teaching ...    Left  \n",
       "34    a catholic activist is suing the justice depar...   Right  \n",
       "40    abortion foes thought roe v. wade'sreversal wo...    Left  \n",
       "83    ( cns news ) -- when asked if a 15-week-old un...   Right  \n",
       "...                                                 ...     ...  \n",
       "8046  u.s. rep. pat schroeder, d-colo., sat on the p...    Left  \n",
       "8072  lawyers who filed a lawsuit challenging the fd...   Right  \n",
       "8073  scolding regular people for contributing to cl...    Left  \n",
       "8084  judicial crisis network president carrie sever...   Right  \n",
       "8085  new hampshire house of representatives voting ...    Left  \n",
       "\n",
       "[1020 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Replace \"pro-life\" or \"pro life\" with \"prolife\"\n",
    "#df['body'] = df['body'].str.replace(r'\\bpro[-\\s]?life\\b', 'prolife', flags=re.IGNORECASE, regex=True)\n",
    "\n",
    "df['body'] = df['body'].str.replace(r'\\bplanned\\s?parenthood\\b', 'plannedparenthood', flags=re.IGNORECASE, regex=True)\n",
    "\n",
    "# Filter rows where \"reproductivehealth\" appears in the \"body\" column\n",
    "#reproductivehealth_rows = df[df['body'].str.contains(r'\\breproductivehealth\\b', flags=re.IGNORECASE, regex=True)]\n",
    "\n",
    "# Replace variations of \"Dobbs v Jackson\" with \"dobbsvjackson\"\n",
    "#df['body'] = df['body'].str.replace(\n",
    "#    r'\\bdobbs\\s(v\\.?|vs\\.?)\\sjackson\\b', \n",
    "#    'dobbsvjackson', \n",
    "#    flags=re.IGNORECASE, \n",
    "#    regex=True\n",
    "#)\n",
    "\n",
    "# Filter rows where \"reproductivehealth\" appears in the \"body\" column\n",
    "plannedparenthood_rows = df[df['body'].str.contains(r'\\bplannedparenthood\\b', flags=re.IGNORECASE, regex=True)]\n",
    "\n",
    "\n",
    "# Display the filtered rows\n",
    "plannedparenthood_rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23d170a2-e9cb-4638-99fe-49c6c2b2775a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  semantic_polarity\n",
      "0   2020-11           0.413202\n",
      "1   2020-12           0.392815\n",
      "2   2021-01           0.327477\n",
      "3   2021-02           0.326952\n",
      "4   2021-03           0.315540\n",
      "5   2021-04           0.337806\n",
      "6   2021-05           0.407169\n",
      "7   2021-06           0.313151\n",
      "8   2021-07           0.295049\n",
      "9   2021-08           0.349062\n",
      "10  2021-09           0.237982\n",
      "11  2021-10           0.211966\n",
      "12  2021-11           0.270067\n",
      "13  2021-12           0.199048\n",
      "14  2022-01           0.357740\n",
      "15  2022-02           0.325508\n",
      "16  2022-03           0.344030\n",
      "17  2022-04           0.399499\n",
      "18  2022-05           0.317165\n",
      "19  2022-06           0.283699\n",
      "20  2022-07           0.302890\n",
      "21  2022-08           0.368290\n",
      "22  2022-09           0.309218\n",
      "23  2022-10           0.332218\n",
      "24  2022-11           0.283501\n",
      "25  2022-12           0.356725\n",
      "26  2023-01           0.369280\n",
      "27  2023-02           0.427606\n",
      "28  2023-03           0.430760\n",
      "29  2023-04           0.432449\n",
      "30  2023-05           0.390187\n",
      "31  2023-06           0.311630\n",
      "32  2023-07           0.062619\n",
      "33  2023-08           0.162513\n",
      "34  2023-09           0.413274\n",
      "35  2023-10           0.000000\n",
      "36  2023-11           0.324677\n",
      "37  2023-12           0.379055\n",
      "38  2024-01           0.418377\n",
      "39  2024-02           0.000000\n",
      "40  2024-03           0.265925\n",
      "41  2024-04           0.359778\n",
      "42  2024-05           0.385616\n",
      "43  2024-06           0.420204\n",
      "44  2024-07           0.340469\n",
      "45  2024-08           0.293651\n",
      "46  2024-09           0.000000\n",
      "47  2024-10           0.497222\n",
      "48  2024-11           0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DebertaV2Tokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "model = AutoModel.from_pretrained('microsoft/deberta-v3-large')\n",
    "\n",
    "# Define the case-insensitive tokenizer function\n",
    "def case_insensitive_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Converts the input text to lowercase before tokenizing.\n",
    "    Ensures consistent tokenization for uppercase and lowercase variations.\n",
    "    \"\"\"\n",
    "    return tokenizer(text.lower())\n",
    "\n",
    "# Function to extract contextual embedding for a specific keyword\n",
    "def get_contextual_embedding(text, keyword, max_len=512):\n",
    "    \"\"\"\n",
    "    Extracts contextual embeddings for a specific keyword within a single chunk of text.\n",
    "    \"\"\"\n",
    "    # Convert text and keyword to lowercase for case-insensitive processing\n",
    "    text = text.lower()\n",
    "    keyword = keyword.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)  # Forward pass\n",
    "\n",
    "    # Retrieve keyword token ID\n",
    "    keyword_id = tokenizer.encode(keyword, add_special_tokens=False)[0]\n",
    "\n",
    "    # Find the index of the keyword\n",
    "    keyword_index = (tokens['input_ids'] == keyword_id).nonzero(as_tuple=True)\n",
    "\n",
    "    if len(keyword_index[1]) > 0:\n",
    "        # Extract and average embeddings for keyword occurrences\n",
    "        keyword_embedding = output.last_hidden_state[0, keyword_index[1], :].mean(dim=0).numpy()\n",
    "        return keyword_embedding\n",
    "    return None\n",
    "\n",
    "# Function to compute semantic polarity between two sets of embeddings\n",
    "def compute_semantic_polarity(left_embeddings, right_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the semantic polarity score between Left and Right embeddings.\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "\n",
    "    for left_emb in left_embeddings:\n",
    "        for right_emb in right_embeddings:\n",
    "            distance = 1 - cosine_similarity(left_emb.reshape(1, -1), right_emb.reshape(1, -1))[0][0]\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "\n",
    "    return total_distance / count if count > 0 else 0\n",
    "\n",
    "# Correctly parse the 'date' column with the specified format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%y-%m-%d', errors='coerce')  # Specify the format 'yy-mm-dd'\n",
    "\n",
    "# Drop rows where the date couldn't be parsed\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Extract the year-month for monthly analysis\n",
    "df['year_month'] = df['date'].dt.to_period('M')  # Creates 'YYYY-MM' format\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Group articles by month and analyze semantic polarity\n",
    "for month, group in df.groupby('year_month'):\n",
    "    left_group = group[group['leaning'] == 'Left']  # Filter Left-leaning articles\n",
    "    right_group = group[group['leaning'] == 'Right']  # Filter Right-leaning articles\n",
    "\n",
    "    # Extract embeddings for the keyword 'prolife'\n",
    "    left_embeddings = [get_contextual_embedding(body, 'plannedparenthood') \n",
    "                       for body in left_group['body'] if get_contextual_embedding(body, 'plannedparenthood') is not None]\n",
    "    right_embeddings = [get_contextual_embedding(body, 'plannedparenthood') \n",
    "                        for body in right_group['body'] if get_contextual_embedding(body, 'plannedparenthood') is not None]\n",
    "\n",
    "    # Compute semantic polarity for the month\n",
    "    sp_score = compute_semantic_polarity(left_embeddings, right_embeddings)\n",
    "    results.append({'month': month.strftime('%Y-%m'), 'semantic_polarity': sp_score})\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "sp_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(sp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7246a499-4430-443e-8b9c-aee85e6e4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path where you want to save the CSV🔥\n",
    "output_file_path = \"/work/Bachelor/results_for_plots/sp_plannedparenthood_df.csv\"\n",
    "\n",
    "# Save the filtered DataFrame as a CSV file\n",
    "sp_df.to_csv(output_file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d0677-8b27-489a-b88b-c20ecf2b4fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
